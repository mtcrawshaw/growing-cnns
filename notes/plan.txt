Cards:
Modify model and growth controller to use adjacency matrix (use this https://github.com/seungwonpark/RandWireNN/blob/master/model/dag_layer.py)
Add skip growth method
Add branching growth method

Future stuff:
Add ImageNet + other datasets
Parallelize across multiple GPUs
Add different block types
Dynamic batch sizes across growth steps using model size estimation (https://github.com/jacobkimmel/pytorch_modelsize)
Fix argument structure for training/evaluation
Convert to package?

Features:
3 point growth methods (options: vanilla, freeze old layers for a time, different learning rates for old and new layers, and sliding weighted average for methods 2, 3)
1 network growth method for point method 1, 3 network growth methods for point method 2, 2 network growth methods for point method 3
Convolution, Resnet basic, Resnet bottleneck, Densenet, Inception module
Dropfilter, Dropblock, stochastic depth, drop path for option 3
Early stopping, fixed number of epochs, training loss convergence


