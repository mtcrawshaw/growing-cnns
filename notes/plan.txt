Cards:
Get static to use CustomConvNet + full-grown architectures
Training settings:
- Stopping criteria
- Hyperparameters
- Batch size?

Future stuff:
Rename growing schemes
Add ImageNet + other datasets
Parallelize across multiple GPUs
Add different block types
Dynamic batch sizes across growth steps using model size estimation (https://github.com/jacobkimmel/pytorch_modelsize)
Fix argument structure for training/evaluation
Convert to package
Add option for CPU only
Clean/break up main

Features:
3 point growth methods (options: vanilla, freeze old layers for a time, different learning rates for old and new layers, and sliding weighted average for methods 2, 3)
1 network growth method for point method 1, 3 network growth methods for point method 2, 2 network growth methods for point method 3
Convolution, Resnet basic, Resnet bottleneck, Densenet, Inception module
Dropfilter, Dropblock, stochastic depth, drop path for option 3
Early stopping, fixed number of epochs, training loss convergence


